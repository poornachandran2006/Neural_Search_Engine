# Neural Search Engine  
### Production-Ready Multi-Document RAG with Map-Reduce Architecture

Neural Search Engine is a full-stack, enterprise-grade Retrieval-Augmented Generation (RAG) system designed for accurate, safe, and explainable document intelligence. It supports strict document scoping, deterministic intent detection, and multi-document reasoning using a hard-enforced Map-Reduce RAG pipeline.

This system is intentionally conservative: it answers **only when information is explicitly present in documents** and returns a clear fallback otherwise.

---

## ğŸš€ Core Features

â€¢ Semantic document search using dense embeddings  
â€¢ Vector storage and retrieval via Qdrant  
â€¢ Current Document and All Documents query modes  
â€¢ Hard-enforced Map-Reduce RAG for multi-document queries  
â€¢ Deterministic intent detection (metadata vs content)  
â€¢ Hallucination-resistant answer generation  
â€¢ File deduplication and ingestion locking  
â€¢ ChatGPT-style UI with persistent chat history  

---

## ğŸ§  System Architecture

### High-Level Architecture Diagram

```mermaid
flowchart LR
    U[User] --> FE[Next.js Frontend]

    FE --> BE[Node.js Backend]

    BE --> ID[Intent Detection]

    ID --> MH[Metadata Handler]
    MH --> FE

    ID --> EMB[Embedding Service]
    EMB --> VS[Qdrant Vector Database]

    VS --> RET[Retriever]

    RET --> LLM1[LLM Single Document]
    RET --> MAP[Map Phase Per Document]

    MAP --> REDUCE[Reduce Phase Merge Answers]

    LLM1 --> RESP[Final Answer]
    REDUCE --> RESP

    RESP --> FE
```

## Query Modes

### Current Document Mode
â€¢ Query is scoped to a single selected document  
â€¢ Prevents cross-document leakage  
â€¢ Requires an explicitly selected document  

### All Documents Mode
â€¢ Searches across all uploaded documents  
â€¢ Ensures balanced retrieval per document  
â€¢ Uses Map-Reduce RAG when multiple documents exist  


neural_search_engine/
â”œâ”€â”€ backend/            # API, RAG logic, intent routing
â”œâ”€â”€ frontend-next/      # Primary Next.js frontend
â”œâ”€â”€ ingestion/          # Python ingestion pipeline
â”œâ”€â”€ infra/              # Docker & infrastructure configs
â”œâ”€â”€ docs/               # Documentation assets
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## Tech Stack

Backend  
â€¢ Node.js + TypeScript  
â€¢ Express  
â€¢ Qdrant Vector Database  
â€¢ LLM API (pluggable)  

Frontend  
â€¢ Next.js 14 (App Router)  
â€¢ TypeScript  
â€¢ Tailwind CSS  

Ingestion  
â€¢ Python  
â€¢ PDF/Text loaders  
â€¢ Chunking with overlap  
â€¢ Vector upserts to Qdrant  

---

## Safety & Correctness Guarantees

âœ” Answers only when information exists in documents  
âœ” Explicit â€œnot foundâ€ responses when missing  
âœ” No hallucinated summaries  
âœ” No silent assumptions  
âœ” Deterministic behavior for identical queries  
âœ” Safe handling of ambiguous prompts  

---

## Local Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/poornachandran2006/Neural_Search_Engine.git
cd Neural_Search_Engine

2. Start Qdrant (Docker Required)
docker run -p 6333:6333 qdrant/qdrant

3. Backend Setup
cd backend
npm install
npm run dev

4. Ingestion Pipeline
cd ingestion
python src/main.py --file path/to/your/document.pdf

5. Frontend Setup
cd frontend-next
npm install
npm run dev


Open the application at:

http://localhost:3000

Environment Variables
Backend (backend/.env)
OPENAI_API_KEY=your_api_key
QDRANT_URL=http://localhost:6333

Frontend (frontend-next/.env.local)
NEXT_PUBLIC_API_BASE_URL=http://localhost:5000

Verified Behavior

â€¢ Current Document queries only use selected document
â€¢ All Documents queries correctly merge information
â€¢ Map-Reduce activates only when required
â€¢ Metadata queries bypass embeddings and LLM
â€¢ Resume and policy documents behave independently
â€¢ Queries like â€œcompareâ€, â€œsummarizeâ€, and â€œlist documentsâ€ work deterministically

Why This Project Matters

This system demonstrates real-world RAG engineering:
â€¢ Multi-document reasoning
â€¢ Controlled LLM usage
â€¢ Safe retrieval pipelines
â€¢ End-to-end system design



